---
title: "Report"

output: 
  html_document:
    toc: true
    toc_float: true
    theme: cosmo
    code_folding: hide
---
# Motivation

Predictors of in-hospital mortality in patients with heart failure admitted to the intensive care unit (ICU) remain unclear. Aimed to lay a solid foundation for Data Science, deepen our understanding of mortality, we conducted a comprehensive research based on the patient's mortality factors dataset. Although this dataset belongs to cross-sectional study, we still can dig out some key patterns that affect mortality and detect abnormal characteristics that indicate the death sign at the early stage. 

# Related work
 
Here are some key areas and examples of related published work:

 1. "Trends in Hospital Mortality Between 1999 and 2017": Studies analyzing long-term trends in hospital mortality rates can provide insights into improvements or challenges in healthcare outcomes over time. Such research often involves large-scale data analysis to identify patterns and changes.

 2. "Risk Factors for Hospital Mortality in Critically Ill Patients": Research that focuses on identifying and understanding specific risk factors associated with increased hospital mortality can inform targeted interventions. This may involve studying critically ill populations and factors such as comorbidities, severity of illness, and response to treatment.

 3. "Effectiveness of Early Warning Systems in Reducing Hospital Mortality": Investigating the impact of early warning systems and rapid response teams on hospital mortality is a common area of interest. This research often assesses the effectiveness of these systems in identifying deteriorating patients and facilitating timely interventions.

 4. "Quality Improvement Initiatives and Their Impact on Hospital Mortality": Published work on quality improvement initiatives explores how hospitals implement changes to improve patient outcomes. This may involve the introduction of new protocols, guidelines, or patient safety measures.

 5. "Role of Health Information Technology in Predicting and Preventing Hospital Mortality": Research examining the integration of health information technology, predictive modeling, and artificial intelligence in predicting and preventing hospital mortality is an evolving field. These studies often assess the accuracy and clinical utility of predictive models.

 6. "Ethical Considerations in End-of-Life Decision-Making and Hospital Mortality": Ethical considerations surrounding end-of-life care, decision-making processes, and their impact on hospital mortality are important topics. Studies in this area may explore the perspectives of healthcare professionals, patients, and families.

 7. "Interdisciplinary Collaboration and its Influence on Hospital Mortality": Research that investigates the role of effective communication and collaboration among healthcare teams in reducing hospital mortality is essential. This may include studies on teamwork, communication strategies, and interdisciplinary approaches to patient care.

 8. "Comparative Effectiveness Research in Hospital Mortality": Comparative effectiveness research aims to compare different treatment modalities and strategies to determine their impact on patient outcomes. Published work in this area may assess the effectiveness of various medical interventions and healthcare delivery models.
 
 
\ \par
\ \par

# Initial questions

What factors have a significant impact on mortality?

What factors account for independent risk factors of in-hospital mortality?

What potential clinical implications are the top three biomedical indices that make the most significant contributions to the specific disease? 

# Data

## Sources

We choose Hospital Mortality Dataset as our dataset in our study. The data can be accessed [here](https://www.kaggle.com/datasets/saurabhshahane/in-hospital-mortality-prediction/data).


## Scaping method

Our main datasets is Zhou, Jingmin et al. (2021), Prediction model of in-hospital mortality in intensive care unit patients with heart failure: machine learning-based, retrospective analysis of the MIMIC-III database, Dryad, Dataset, https://doi.org/10.5061/dryad.0p2ngf1zd, which covers the patients' various indexes comprehensively, including 51 variables such as Blood Pressure and BMI. 

We categorized the dataset into following aspects: 

1. **Demographic characteristics:** age at the time of hospital admission, sex, ethnicity, weight, and height; 

2. **Vital signs:** heart rate, HR, systolic blood pressure SBP, diastolic blood pressure DBP, mean blood pressure, respiratory rate, body temperature, saturation pulse oxygen SPO2, urine output in first 24 h; 

3. **Comorbidities:** hypertension, atrial fibrillation, ischemic heart disease, diabetes mellitus, depression, hypoferric anemia, hyperlipidemia, chronic kidney disease CKD, and chronic obstructive pulmonary disease COPD; 

4. **Laboratory variables:** hematocrit, red blood cells, mean corpuscular hemoglobin MCH, mean corpuscular hemoglobin concentration MCHC, mean corpuscular volume MCV, red blood cell distribution width RDW, platelet count, white blood cells, neutrophils, basophils, lymphocytes, prothrombin time PT, international normalized ratio INR, NT-proBNP, creatine kinase, creatinine, blood urea nitrogen BUN glucose, potassium, sodium, calcium, chloride, magnesium, the anion gap, bicarbonate, lactate, hydrogen ion concentration pH, partial pressure of CO2 in arterial blood, and LVEF


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

```{r,message=FALSE, warning=FALSE}
# Load the data and preprocess it
  mortality_data <- read_csv("mortality_data_cleaned.csv") %>%
  janitor::clean_names() %>%
  drop_na(outcome) 

```


## Cleaning

**Data cleaning problems:**

1. Too many NA values: This issue introduces a layer of complexity, demanding meticulous scrutiny to discern patterns, trends, and potential biases. 

2. Too many categorical variables/ binary variables: Similar to an intricate cast of characters in a narrative, these variables contribute to the richness of the data but also pose challenges for analysis. 

3. Multidimensionality: A dataset with numerous dimensions can overwhelm analytical processes.

**Our solutions:**

1. Change NA values to the mean value of each variables;

2. Binarization: This process involves transforming numerical or categorical variables into a binary representation, typically consisting of 0s and 1s.

3. Dimensionality Reduction using Principle Component Analysis (PCA); Or feature selection, become essential to distill the dataset into its most salient components.

```{r, message=FALSE}
# Define a function to convert multiple columns to factors
convert_to_factor <- function(df, columns) {
  df[columns] <- lapply(df[columns], factor)
  return(df)
}

# Load the data and preprocess it
mortality_data <- read_csv("mortality_data.csv") %>%
  janitor::clean_names() %>%
  drop_na(outcome) %>%
  convert_to_factor(., c("group", "gendera", "outcome", "hypertensive", 
                         "atrialfibrillation", "chd_with_no_mi", "diabetes", 
                         "deficiencyanemias", "depression", "hyperlipemia", 
                         "renal_failure", "copd")) %>%
  rename(gender = gendera)  # Rename gendera to gender after conversion
```

Algorithms that find association patterns require that the data be in the form of binary attributes. Thus, it is often necessary to transform a continuous attribute into a categorical attribute (discretization), and both continuous and discrete attributes may need to be transformed into one or more binary attributes (binarization). 

Additionally, if a categorical attribute has a large number of values (categories), or some values occur infrequently, then it can be beneficial for certain data mining tasks to reduce the number of categories by combining some of the values. 

```{r}
# Imputing numerical columns with mean value 

numerical_columns <- sapply(mortality_data, is.numeric)
mortality_data[numerical_columns] <- lapply(mortality_data[numerical_columns], function(x) {
   ifelse(is.na(x), mean(x, na.rm = TRUE), x)
})


write_csv(mortality_data, "mortality_data_cleaned.csv")
```

After data cleaning, our dataset change into high quality data, with:

1. No duplicates, ensuring that there are no duplicates is a fundamental aspect of data quality and integrity;

2. No missing values, guaranteeing there are no missing values involves thorough data cleaning and preprocessing steps;

3. No noise and outliers, contributing to the robustness and reliability of statistical models, machine learning algorithms, and other data-driven analyses;

4. Consistency, ensuring data consistency is essential for making reliable interpretations, conducting meaningful analyses, and supporting effective decision-making. 


```{r defaults, echo = FALSE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(data.table)
library(survival)
library(plotly)
library(corrplot)
library(FactoMineR)
library(factoextra)
library(rpart)
library(rpart.plot)
library(randomForest)
library(party)

# read the data
mortality_data <- 
  read_csv("mortality_data_cleaned.csv")

# Change the factor variables for EDA 

col_names <- c("group", "outcome", "hypertensive", 
                         "atrialfibrillation", "chd_with_no_mi", "diabetes", 
                         "deficiencyanemias", "depression", "hyperlipemia", 
                         "renal_failure", "copd")

mortality_data[,col_names] <- lapply(mortality_data[,col_names] , factor)

# Manually recoding factors to their meaningful character values
mortality_data_EDA <- mortality_data %>%
  mutate(
    group = recode(group, `1` = "Group 1", `2` = "Group 2"),
    gender = recode(gender, `1` = "Male", `2` = "Female"),
    outcome = recode(outcome, `0` = "Alive", `1` = "Death")
  ) %>%
  mutate_if(is.factor, as.character) # Converts all remaining factors to characters

# Function to recode factor variables to 'Yes' or 'No'
convert_factors_to_yes_no <- function(df, comorbidity_columns) {
  df <- df %>%
    mutate(across(all_of(comorbidity_columns), ~ ifelse(as.character(.) == "1", "Yes", "No")))
  return(df)
}

# List of comorbidity columns to convert
comorbidity_columns <- c("hypertensive", "atrialfibrillation", "chd_with_no_mi", 
                         "diabetes", "deficiencyanemias", "depression", 
                         "hyperlipemia", "renal_failure", "copd")

# Apply the function to the mortality_data_EDA dataframe
mortality_data_EDA <- convert_factors_to_yes_no(mortality_data_EDA, comorbidity_columns)

## Preliminary Study
#load mortality dataset
mort_data =
  read_csv("mortality_data_cleaned.csv") |> 
  janitor::clean_names() |>
  select(-group, everything())

convert_to_factor <- function(df, columns) {
  df[columns] <- lapply(df[columns], factor)
  return(df)
}

mort_tidy = 
  mort_data |>
  mutate(outcome = recode(outcome, `0` = "Alive", `1` = "Death")) |>
  convert_to_factor(c("group", "gender", "outcome", "hypertensive", 
                      "atrialfibrillation", "chd_with_no_mi", "diabetes", 
                      "deficiencyanemias", "depression", "hyperlipemia", 
                      "renal_failure", "copd"))

#select patient comorbidities
com_tidy =
  mort_tidy |>
  select(outcome, hypertensive:copd)

#select patient vital signs
sign_tidy =
  mort_tidy |>
  select(outcome, heart_rate:ef)
```

# Additional analysis

## Exploratory Data Analysis

The EDA conducted on heart failure patients in the Intensive Care Unit (ICU) provides valuable insights into the demographic, physiological, and clinical factors influencing mortality rates.

## Demographic Characteristics 

The exploration of demographic factors such as gender and age provides critical insights into their potential impact on the outcomes of heart failure patients admitted to the Intensive Care Unit (ICU). This report section presents an analysis of in-hospital mortality rates categorized by gender and age groups. 

### Gender-Based Mortality Analysis

```{r}
mortality_data_EDA %>%
  group_by(gender) %>%
  summarise(
    Count = n(), # total number of entries for each gender
    Alive = sum(outcome == "Alive"), # number of outcomes with value 0
    Death = sum(outcome == "Death"), # number of outcomes with value 1
    Percentage = Death/Count
  ) %>%
  knitr::kable(digits = 3)  
```

The dataset comprised 618 females and 558 males. The mortality rate among female patients was 12.8%, while the mortality rate for male patients was slightly higher at 14.3%. 


### Age-Based Mortality Analysis

```{r}
# Define age intervals
age_breaks <- c(-Inf, 20, 40, 60, 80, Inf)
age_labels <- c('Under 20', '20-40', '40-60', '60-80', 'Over 80')

# Create age groups and summarize outcomes
mortality_data_EDA %>%
  mutate(Age_group = cut(age, breaks = age_breaks, labels = age_labels, right = FALSE)) %>%
  group_by(Age_group) %>%
  summarise(Count = n(),
            Alive = sum(outcome == "Alive", na.rm = TRUE),
            Death = sum(outcome == "Death", na.rm = TRUE),
            Percentage = Death/(Alive + Death)) %>%
  knitr::kable(digits = 3)  

```

It was observed that patients under 20 years had a 0% mortality rate. Patients aged 20-40 years had a mortality rate of 6.2%, which increased with age. The 40-60 and 60-80 age groups had mortality rates of 12.7% and 11.6%, respectively. Notably, the group over 80 years had the highest mortality rate at 15.9%, underscoring the increased vulnerability among the elderly population.

```{r}
# Distribution of Age
ggplot(mortality_data_EDA, aes(x = age)) +
  geom_histogram(binwidth = 3, fill = "steelblue", color = "black") +
  ggtitle("Age Distribution")

```

The histogram shows the age distribution of patients with heart failure admitted to the ICU, and it is highly left-skewed. It shows that the bulk of patients falls within the middle-aged to elderly category, with a significant concentration between ages 70 and 90. This suggests that heart failure is more prevalent or more severe, warranting ICU admission, in this age demographic.

### Gender & Age Bivariate Analysis

```{r}
# Bivariate Analysis
ggplot(mortality_data_EDA, aes(x = gender, y = age, fill = outcome)) +
  geom_boxplot() +
  ggtitle("Age Distribution by Gender and Outcome")

```

The boxplots show the age distribution for different genders and outcomes. We see that for both genders, the age distribution for patients who did not survive (outcome 1) is slightly higher. This could indicate that older age is a risk factor for mortality in this patient group.

### BMI by Gender and Outcome

The histogram below illustrates the distribution of Body Mass Index (BMI) across patients with heart failure, categorized by gender and outcome (Alive vs. Death).

```{r, fig.width = 9, fig.height = 6}
# BMI by Gender and Outcome
ggplot(mortality_data_EDA %>% drop_na(bmi), aes(x = bmi, fill = as.factor(outcome))) +
  geom_histogram(binwidth = 4, position = "dodge") +
  facet_wrap(~gender) +
  ggtitle("BMI by Gender and Outcome")

```

It shows a wide range of BMI values for both genders, with no clear pattern indicating that BMI is distinctly associated with the outcome. However, there are more data points for BMI for outcome "Alive", which may suggest a higher survival rate for patients with a lower BMI, but further statistical analysis is needed.

## Comorbidities Analysis

Analyzing the distribution of comorbidities by outcome can identify factors that influence mortality in patients with heart failure. It helps to understand the impact of comorbid conditions on patient survival and could inform targeted interventions. The stacked bar chart below presents the distribution of various comorbidities among heart failure patients in the ICU, differentiated by patient outcomes of survival and death. 

```{r, fig.width = 9, fig.height = 6, message = FALSE, warning = FALSE}
# Analyzing the presence of comorbidities by outcome
comorbidities <- c("hypertensive", "diabetes", "deficiencyanemias", "depression", "renal_failure", "copd", "hyperlipemia")

# Melt the data for easier plotting
mortality_long <- melt(mortality_data_EDA, id.vars = "outcome", measure.vars = comorbidities)

# Plotting comorbidities by outcome
comorbidity_plot <- ggplot(mortality_long, aes(x = variable, fill = as.factor(value))) +
  geom_bar(position = "fill") +
  facet_wrap(~outcome) +
  labs(x = " ", y = "Count", fill = "Presence") +
  ggtitle("Distribution of Comorbidities by Outcome") +
  scale_y_continuous(labels = scales::percent) +
  theme(axis.text.x = element_text(angle = 50, hjust = 1))

interactive_plot <- ggplotly(comorbidity_plot)

interactive_plot
```

Observations based on the chart include:

1. **Prevalence of Comorbidities**: The chart shows that certain comorbidities such as hypertension, diabetes, and hyperlipemia are common among heart failure patients. This indicates that these conditions frequently co-occur with heart failure, which could compound the complexity of patient management.

2. **Impact on Mortality**: The chart indicates a higher 'Yes' proportion in the 'Alive' outcome, which suggests that having the comorbidity did not necessarily correlate with a higher mortality risk for that specific condition within this patient sample. However, there could be a survival bias where patients with certain comorbidities survive long enough to be included in the study, whereas those severely affected may not have been admitted or were not included in the dataset. 

3. **Variable Significance**: The difference between two groups underscores the importance of considering comorbidities in predictive modeling for mortality. Further analysis could involve investigating the reasons behind the survival rates of patients with specific comorbidities. 


## Lab Results Analysis

This section offers a quantitative glimpse into potential biological markers that could contribute to our predictive model, thus enhancing our ability to conduct further analysis and understand mortality more deeply. 

### Hematocrit Levels by Outcome

```{r}
# Visualize Hematocrit Levels by Outcome with a violin plot
ggplot(mortality_data_EDA, aes(x = outcome, y = hematocrit, color = outcome)) +
  geom_violin() +
  labs(title = "Hematocrit Levels by Outcome", x = "Outcome", y = "Hematocrit (%)") +
  theme_minimal()

```

The plot illustrates a comparison of hematocrit levels in patients who were admitted to the ICU with heart failure and subsequently either survived (Alive) or did not (Death). There doesn't seem to be a clear separation between the two outcomes based on hematocrit levels alone. The peak of the distribution for the deceased patients appears to be slightly lower than that of the survivors. 

### Heart Rate Distribution by Outcome

The density plot below illustrates the distribution of heart rate among patients with heart failure, categorized by the outcome of either survival (Alive) or death (Death). 

```{r}
# Heart Rate by Outcome
ggplot(mortality_data_EDA, aes(x = heart_rate, fill = as.factor(outcome))) +
  geom_density(alpha = 0.7) +
  labs(title = "Heart Rate Distribution by Outcome", x = "Heart Rate", y = "Density") +
  theme_minimal()
```

The density plot for heart rate shows that the distribution of heart rates for patients who did not survive is slightly shifted towards the higher end compared to those who survived. This could suggest that a higher heart rate is associated with a higher risk of mortality.

### Blood Pressure by Outcome
```{r}
# Blood Pressure by Outcome
ggplot(mortality_data_EDA, aes(x = systolic_blood_pressure, y = diastolic_blood_pressure, color = as.factor(outcome))) +
  geom_point(alpha = 0.5) +
  ggtitle("Blood Pressure by Outcome")

```

This scatterplot shows systolic vs. diastolic blood pressure colored by outcome. The points are widely spread, indicating variability in blood pressure readings across the patient population. There doesn't seem to be a clear separation between the two outcomes based on blood pressure alone.

### Other Lab Results Analysis

This section contains the visualization for some other lab results, like the distributions for creatinine and BUN levels. More visualization can be found in the shiny app section. 

```{r, figures-side, fig.show="hold", out.width="50%"}
par(mar = c(4, 4, .1, .1))

# Creatinine levels by outcome
ggplot(mortality_data_EDA %>% drop_na(creatinine), aes(x = creatinine, fill = as.factor(outcome))) +
  geom_density(alpha = 0.5) +
  ggtitle("Creatinine Levels by Outcome") 

# Urea nitrogen levels by outcome
ggplot(mortality_data_EDA %>% drop_na(urea_nitrogen), aes(x = urea_nitrogen, fill = as.factor(outcome))) +
  geom_density(alpha = 0.5) +
  ggtitle("Blood Urea nitrogen Levels by Outcome")
```


```{r, fig.show="hold", out.width="50%"}
par(mar = c(4, 4, .1, .1))

# Leucocyte count by outcome
ggplot(mortality_data_EDA %>% drop_na(leucocyte), aes(x = leucocyte, fill = as.factor(outcome))) +
  geom_density(alpha = 0.5) +
  ggtitle("Leucocyte Count by Outcome")

# Glucose levels by outcome
ggplot(mortality_data_EDA %>% drop_na(glucose), aes(x = glucose, fill = as.factor(outcome))) +
  geom_density(alpha = 0.5) +
  ggtitle("Glucose Levels by Outcome")

```

Observations based on the plots include:

- **Blood Urea Nitrogen Levels by Outcome**: Patients with higher blood urea Nitrogen levels seem to have a poorer outcome, as indicated by the longer tail in the distribution for non-survivors. High blood urea nitrogen levels can be indicative of renal insufficiency or failure, which is a known risk factor for mortality.
- **Leucocyte Count by Outcome**: The density plot shows a higher peak for leucocyte count among survivors (outcome 0) compared to non-survivors (outcome 1). However, there is a long tail in the distribution for non-survivors, suggesting that some patients who did not survive had very high leucocyte counts, which could indicate severe infection or systemic stress.









## Preliminary Study


### Correlation between Vital Signs


```{r fig.width = 10, fig.height = 12}
#show correlation
corrplot(cor(sign_tidy |> select(-outcome)), type = "upper", order = "AOE", diag = FALSE, title = "Correlation Plot of Variables", cex.main = 1.5, mar = c(0, 0, 1, 0))
```

The correlation plot illustrates the relationships between vital signs pairwise. Darker colors indicate stronger correlations, where blue signifies positive correlations and red signifies negative ones.  

Strong positive correlations were observed between variables such as `rbc` and `hematocrit`, `mcv` and `mch`, `inr` and `pt`, `pco2` and `bicarbonate`. Conversely, a strong negative correlation was found between `neutrophils` and `lymphocyte`. Strong positive correlations often suggest the potential for dimension reduction by merging these variables during selection. 


### CART and rf: Dimension Reduction and Core Variables Analysis


#### Analysis of Patient comorbidities Variables


```{r}
#decision tree for comorbidities
tree_com = ctree(outcome ~ ., data = com_tidy)
plot(tree_com, main = "Decision Tree for Comorbidities")
```

The decision tree employed variables `renal_failure` and `atrialfibrillation` to predict the outcome, showing strong evidence (p<0.05) for selecting these two variables.  

However, the probability distribution at the nodes suggests that this decision-making approach might not be sufficiently reliable. This could be due to the similar effects of each variable, making it challenging to determine the next variable for refining the decision. Alternatively, it might indicate insufficient evidence to support the selection of other variables (p-values not meeting the criteria).  

Furthermore, the binary nature of these patient comorbidities variables posed certain challenges in constructing the classification tree, making characteristics of the variables hard to analyze.  

```{r}
#random forest for comorbidities
rf_com = randomForest(outcome ~ ., data = com_tidy)
importance(rf_com) |> 
  as.data.frame() |> 
  mutate("Mean Decrease in Gini" = MeanDecreaseGini) |>
  arrange(-MeanDecreaseGini) |> 
  select(-MeanDecreaseGini) |>
  head(8) |> 
  knitr::kable(digits = 3)
varImpPlot(rf_com, main = "Random Forests for Comorbidities")
```

The random forest yielded a list of variables ranked by their importance based on `Mean Decrease in Gini`, a parameter calculated by assessing the influence of variables on the nodes of the classification tree.  

In descending order of importance, the top 5 variables are: `hyperlipemia`, `atrialfibrillation`, `renal_failure`, `deficiencyanemias`, `diabetes`.  


#### Analysis of Patient Vital Signs Variables


```{r fig.width = 12, fig.height = 8}
tree_sign = ctree(outcome ~ ., data = sign_tidy)
plot(tree_sign, main = "Decision Tree for Vital Signs")
```

The analysis of continuous variables related to vital signs resulted in the successful establishment of classification criteria by the decision tree, incorporating a total of 7 variables. It formed a 4-level classification structure with reasonably distributed probabilities across nodes.  

The primary level classification variable is `anion_gap`. The secondary level classification variables are `blood_calcium`, `bicarbonate`. The tertiary level classification variables are `lactic_acid`, `leucocyte`, `respiratory_rate`. The forth level classification variable is `pt`.  

```{r fig.width = 8, fig.height = 8}
#random forest for vital signs
rf_sign = randomForest(outcome ~ ., data = sign_tidy)
importance(rf_sign) |> 
  as.data.frame() |>
  mutate("Mean Decrease in Gini" = MeanDecreaseGini) |>
  arrange(-MeanDecreaseGini) |> 
  select(-MeanDecreaseGini) |>
  head(8) |> 
  knitr::kable(digits = 3)
varImpPlot(rf_sign, main = "Random Forests for Vital Signs")
```

Ranked by the `Mean Decrease in Gini`, the top 5 variables are: `anion_gap`, `bicarbonate`, `lactic_acid`, `lymphocyte`, `leucocyte`.  

The results obtained from the random forest exhibit a overlap with the decision tree outcomes. In fact, random forest is an extension of decision trees that address over-fitting issues inherent in decision tree model. Therefore, we tend to prioritize the key variables identified by the random forest.  


## PCA: Dimension Reduction


#### Dimension Reduction for Patient Vital Signs Variables


```{r}
#pca using R build-in function prcomp()
#pca_res_sign = prcomp(sign_tidy |> select(-outcome), scale. = TRUE)
#summary(pca_res_sign)

#pca using packages `FactoMineR`, `factoextra`
#PCA() function would do the data standardization automatically.
#keep 5 dimensions as result.
res_pca_sign = PCA(sign_tidy |> select(-outcome), scale.unit = TRUE, graph = FALSE)

#`variance.percent` explains the percentage of change. 70% would be adequate.
get_eigenvalue(res_pca_sign) |> 
  as.data.frame() |> 
  filter(cumulative.variance.percent < 61) |> 
  mutate(
    "Eigenvalue" = eigenvalue,
    "Variance Percent" = variance.percent,
    "Cumulative Variance Percent" = cumulative.variance.percent
  ) |>
  select(-eigenvalue, -variance.percent, -cumulative.variance.percent) |>
  knitr::kable(digits = 3)
```

The `eigenvalue` connects with the amount of data variance explained by each principal component (PC). A higher eigenvalue indicates a greater proportion of data variance that can be explained (`variance.percent`).  
  
The PCs are arranged in descending order based on the eigenvalues, indicating that the PCs positioned earlier correspond to the directions with the greatest data variation. 

Typically, we can limit the number of dimensions/PCs by controlling the `cumulative.variance.percent`. For instance, in this case, we aimed to explain 60% of the total data variance in the model, resulting in the selection of 10 dimensions.  

```{r}
#scree plot
#visualize the contributions of each dimension.
fviz_screeplot(res_pca_sign, addlabels = TRUE, ylim = c(0, 15), main = "Scree Plot of Dimension's Contributions", font.main = 15)
```

Another approach involves examining the **scree plot**, which visualizes the ability of each dimension to explain the overall variance in the data. When the explanatory power begins to decrease and flatten out, it suggests diminishing returns in terms of dimension reduction. This is often considered a bottleneck point, and it's generally reasonable to halt operations.  

In this instance, after introducing the third dimension, there's a limited change in the explanatory power as the dimensions increase. It might be considered to select the first three dimensions for subsequent model analysis. However, due to the relatively small absolute size of the overall explainable variance, the cumulative explained variance of the first three dimensions falls short of 50%. Therefore, in this case, this approach might not be suitable. 

```{r}
#result_cos2
var_sign <- get_pca_var(res_pca_sign) 
var_sign$cos2 |>
  #for pca_var object '$' is necessary for exacting results
  as.data.frame() |> 
  arrange(-Dim.1) |> 
  head(10) |> 
  knitr::kable(digits = 3)
```

In a factor map, a higher `cos2` value close to 1 indicates a stronger relationship between the variable and the principal component, signifying higher quality.  

```{r fig.width = 10, fig.height = 15}
#shows the quality of variables.
corrplot(var_sign$cos2, is.corr = FALSE, title = "Variables Quality - PCs Correlation Plot", mar = c(0, 0, 1, 0), cl.align.text = "l")
```

The correlation plot visualizes the indicator `cos2`, representing the variable-PC correlation. The model selected 5 dimensions for plotting the correlation map.  
  
For `dim.1`, the variable with the highest correlation are `bicarbonate`, `anion_gap`, `urea_nitrogen`. For `dim.2`, the variable with the highest correlation are `mch`, `rbc`.  

```{r}
#result_contri
var_sign$contrib |> 
  #for pca_var object '$' is necessary for exacting results
  as.data.frame() |> 
  arrange(-Dim.1) |> 
  head(10) |>
  knitr::kable(digits = 3)
```

The `var$contrib` provides the contribution magnitude of variables to each PC. Subsequently, visualizations can be generated based on the contribution values of variables to PCs. However, due to limitations in two-dimensional plotting, typically only the first two dimensions can be visualized.  

```{r fig.width = 10, fig.height = 15}
#shows the contribution in different dimensions.
corrplot(var_sign$contrib, is.corr = FALSE, title = "Contributions - PCs Correlation Plot", mar = c(0, 0, 1, 0), cl.align.text = "l")
```

Correlation plot can draw emphasis on the highest contribution variables in each dimension.  

```{r}
fviz_contrib(res_pca_sign, choice = "var", axes = 1, ylim = c(0, 15), top = 8)
fviz_contrib(res_pca_sign, choice = "var", axes = 1:5, ylim = c(0, 7.5), top = 8)
```

Generate bar charts displaying the contribution magnitude for each dimension, with a red dashed line indicating the expected average contribution.For a given PC, if a variable's contribution exceeds this threshold, it can be considered important within that PC or group of PCs.  

```{r}
#based on above results (`var_com$coord`)
#basically,
#(a) positive correlated variables are grouped together,
#(b) negative correlated variables are located on opposite sides of the origin,
#(c) the distance between the variable and the origin measures the quality of the variable. Variables that are far from the origin are well represented.

pca_coord = PCA(sign_tidy |> select(anion_gap, blood_calcium, bicarbonate, lactic_acid, leucocyte, lymphocyte, neutrophils, rbc, urea_nitrogen), scale.unit = TRUE, graph = FALSE)

fviz_pca_var(pca_coord, col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07")) |> ggplotly()
```

We've created visualizations showcasing the importance of variables concerning the two most significant dimensions.  

`cos2` is used to assess the variables' importance, represented by the distance between the arrow endpoint and the origin of the plot. Variables closer to the correlation circle with r=1 indicate higher representation on the factor map, signifying greater importance. Variables closer to the center of the plot are less important for the selected principal components.  

Additionally, variables with larger contributions can be highlighted on the correlation plot by coloring them based on the values from `var_sign$contrib`.  

```{r}
#colored individuals
fviz_pca_ind(res_pca_sign,
             geom.ind = "point",
             col.ind = as.character(sign_tidy$outcome),
             palette = c("#00AFBB", "#FC4E07"),
             addEllipses = TRUE,
             legend.title = "Outcome")
```

Based on the outcome (`Alive` or `Death`), the data points were grouped and color-coded accordingly. Ellipses were generated to encapsulate the sample points for different groups (alive and death), showcasing the analysis using the two principal components.  





=======




## Regression

In this section, we want to generate a generalized linear model to see the relationship between patients' mortality status and predictive variables.

## Data Pre-processing

Firstly, to prepare the data for the regression, the data types of categorical variables will be transformed to factor. This subset of variables includes group (1 denoting the derivation group, 2 denoting the validation group), gender (1 denoting male, 2 denoting female), outcome (0 denoting alive, 1 denoting death), and the remaining comorbidity variables (0 denoting not having the disease, 1 denoting having the disease)

```{r, message = FALSE}
library(tidyverse)
library(stargazer)
library(ROCR)

convert_to_factor <- function(df, columns) {
  df[columns] <- lapply(df[columns], factor)
  return(df)
}

mortality_cleaned = 
  read_csv("mortality_data_cleaned.csv") |>
  janitor::clean_names()  |>
  convert_to_factor(c("group", "gender", "outcome", "hypertensive", 
                      "atrialfibrillation", "chd_with_no_mi", "diabetes", 
                      "deficiencyanemias", "depression", "hyperlipemia", 
                      "renal_failure", "copd"))
```

### Generalized linear model

Due to the response variable (outcome) is binary variable, a generalized linear model (GLM) is chosen for regression analysis.

#### Generalized Linear Model for Complete Dataset (Group1 and Group2)

Fit a logistic regression model using all patients data (including both the derivation and validation groups) and use the stepwise method to select predictors to be included in the model.

```{r, message = FALSE, results = "hide"}
mortality_complete = 
  mortality_cleaned |>
  select(-group, -id)
glm_complete = glm(outcome ~ ., data = mortality_complete, family = binomial(link = logit))
stepwise_complete = step(glm_complete)
```

```{r}
stepwise_complete |>
  broom::tidy() |>
  select(term, estimate, p.value) |>
  knitr::kable(
    caption = "Estimate and P-value of Generalized Linear Model Using All Patients Data", 
    col.names = c("Predictor", "Estimate", "P-value"),
    digits = 3
  ) |>
    kableExtra::kable_styling(
    "basic",
    full_width = FALSE,
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    position = "center", 
    font_size = 16, 
    latex_options = c("hold_position")
  )
```

The model fitted using data from all patients is:

$$
outcome \sim age+deficiencyanemias1+renal\_failure1+copd1\\
+heart\_rate+diastolic\_blood\_pressure+respiratory\_rate\\
+temperature+sp\_o2+urine\_output+mch+mchc+rdw+leucocyte\\
+platelets+lymphocyte+creatinine+urea\_nitrogen\\
+blood\_potassium+blood\_calcium+anion\_gap+magnesium\_ion\\
+bicarbonate+pco2
$$

It is worth noting that among the comorbidity variables, only deficiencyanemias, renal_failure, and copd are selected by model, and the estimates for all three are negative. This suggests that the occurrence of these three comorbidities is associated with a decrease in mortality, which is counterintuitive. This phenomenon may be attributed to the correlation between the onset of these diseases and early diagnosis and treatment. Early detection and treatment of these diseases might reduce the risk of death. Alternatively, the occurrence of these diseases may prompt patients to pay more attention to their health and take actions to improve their health conditions, thereby enhancing their survival rates.
In the remaining variables, the most significant six are blood_calcium, anion_gap, platelets, heart_rate, urea_nitrogen, and pco2. They may have the potential to be important physiological indicators worth monitoring closely in ICU patients.

#### Generalized Linear Model for Derivation Dataset (Group1)

Fit a logistic regression model using patients data from group1 (derivation group) and use the stepwise method to select predictors to be included in the model.

```{r, message = FALSE, results = "hide"}
mortality_derivation =
  mortality_cleaned |>
  filter(group == 1) |>
  select(-group, -id) 
glm_derivation = glm(outcome ~ ., data = mortality_derivation, family = binomial(link = logit))
stepwise_derivation = step(glm_derivation)
```

```{r}
stepwise_derivation |>
  broom::tidy() |>
  select(term, estimate, p.value) |>
  knitr::kable(
    caption = "Estimate and P-value of Generalized Linear Model Using Patients Data from Derivation Group", 
    col.names = c("Predictor", "Estimate", "P-value"),
    digits = 3
  ) |>
    kableExtra::kable_styling(
    "basic",
    full_width = FALSE,
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    position = "center", 
    font_size = 16, 
    latex_options = c("hold_position")
  )
```

The model fitted using data from derivation group is:

$$
outcome\sim age+gender2+hypertensive1+atrialfibrillation1+diabetes1\\ +deficiencyanemias1+hyperlipemia1+renal\_failure1+copd1+heart\_rate\\
+diastolic\_blood\_pressure+sp\_o2+mcv+rdw+platelets+lymphocyte\\
+creatinine+urea\_nitrogen+blood\_calcium+anion\_gap+lactic\_acid+pco2
$$

Among all the predictors selected, there are 7 comorbidity variables: hypertensive, atrialfibrillation, diabetes, deficiencyanemias, hyperlipemia, renal_failure and copd. All of them have negative estimates except for atrialfibrillation. This indicates that the occurrence of other complications leads to a decrease in mortality (as discussed in the complete model section), but the presence of atrial fibrillation is a noteworthy sign of mortality and should be closely monitored, especially in the ICU. In the remaining variables, the most significant five are blood_calcium, anion_gap, heart_rate, urea_nitrogen, and creatinine. All of them excapt for creatinine are also among the most significant factors in the complete model, and these four should be particularly noteworthy.

#### Model Evaluation on Validation Group (Group2)

For the model generated using derivation group data, validate it on the validation group and plot Receiver Operating Characteristic (ROC) Curve.

```{r, massage = FALSE}
mortality_validation =
  mortality_cleaned |>
  filter(group == 2) |>
  select(-group, -id) 
predict = predict(stepwise_derivation, newdata = mortality_validation, type = "response")
pred = prediction(predictions = predict, labels = mortality_validation$outcome)
roc = performance(pred, "tpr", "fpr")
auc = performance(pred, measure = "auc")@y.values[[1]]

plot(roc, colorize = F)
title(main = "The ROC Curve of Generalized Linear Model")
```

The AUC of the model on validation dataset: 0.7757475. According to the ROC plot and AUC value, the predictive performance of the model on the validation group data is generally satisfactory.

# Discussion

## 1. Findings

### 1) Generalized Linear Model for Complete Dataset 

The complete model is fitted using patients data from both group1 (derivation group) and group2 (validation group). A stepwise method is conducted to select predictors to be included in the model.

The expression of complete model is:

$$
outcome \sim age+deficiencyanemias1+renal\_failure1+copd1\\
+heart\_rate+diastolic\_blood\_pressure+respiratory\_rate\\
+temperature+sp\_o2+urine\_output+mch+mchc+rdw+leucocyte\\
+platelets+lymphocyte+creatinine+urea\_nitrogen\\
+blood\_potassium+blood\_calcium+anion\_gap+magnesium\_ion\\
+bicarbonate+pco2
$$

- It is worth noting that among the comorbidity variables, only deficiencyanemias, renal_failure, and copd are selected by model, and the estimates for all three are negative. This suggests that the occurrence of these three comorbidities is associated with a decrease in mortality, which is counterintuitive. This phenomenon may be attributed to the correlation between the onset of these diseases and early diagnosis and treatment. Early detection and treatment of these diseases might reduce the risk of death. Alternatively, the occurrence of these diseases may prompt patients to pay more attention to their health and take actions to improve their health conditions, thereby enhancing their survival rates.
- In the remaining variables, the most significant six are blood_calcium, anion_gap, platelets, heart_rate, urea_nitrogen, and pco2. They may have the potential to be important physiological indicators worth monitoring closely in ICU patients.

### 2) Generalized Linear Model for Derivation Dataset

The derivation model is fitted using patients data from group1 (derivation group). A stepwise method is conducted to select predictors to be included in the model.

The expression of derivation model is:

$$
outcome\sim age+gender2+hypertensive1+atrialfibrillation1+diabetes1\\ +deficiencyanemias1+hyperlipemia1+renal\_failure1+copd1+heart\_rate\\
+diastolic\_blood\_pressure+sp\_o2+mcv+rdw+platelets+lymphocyte\\
+creatinine+urea\_nitrogen+blood\_calcium+anion\_gap+lactic\_acid+pco2
$$

- Among all the predictors selected, there are 7 comorbidity variables: hypertensive, atrialfibrillation, diabetes, deficiencyanemias, hyperlipemia, renal_failure and copd. All of them have negative estimates except for atrialfibrillation. This indicates that the occurrence of other complications leads to a decrease in mortality (as discussed in the complete model section), but the presence of atrial fibrillation is a noteworthy sign of mortality and should be closely monitored, especially in the ICU. 
- In the remaining variables, the most significant five are blood_calcium, anion_gap, heart_rate, urea_nitrogen, and creatinine. All of them excapt for creatinine are also among the most significant factors in the complete model, and these four should be particularly noteworthy.

### 3) Model Evaluation on Validation Group

The derivation model is validated on the validation group (group2) and a Receiver Operating Characteristic (ROC) Curve is plotted.

The AUC of the model on validation dataset: 0.7757475. According to the ROC plot and AUC value, the predictive performance of the model on the validation group data is generally satisfactory.



## 2. Demographic Insights

The gender-based mortality analysis revealed a slightly higher mortality rate in male patients (14.3%) compared to females (12.8%). Age-wise, the highest mortality rate was observed in patients over 80 years (15.9%), highlighting the increased vulnerability among the elderly.

## 3. Future Directions

The insights gained from this analysis can guide further detailed statistical analysis and predictive modeling. Especially, investigating the reasons behind the observed trends and how they interact to influence patient outcomes could be a valuable next step.

This exploratory analysis sets the stage for more in-depth statistical analysis and modeling, aiming to enhance the understanding and prediction of mortality rates in heart failure patients admitted to the ICU.
