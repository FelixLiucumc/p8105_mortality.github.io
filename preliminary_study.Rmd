---
title: "Preliminary Study"
author: "Yifei Liu"
date: 2023/11/25
output: 
  html_document:
    toc: true
    toc_float: true
    theme: cosmo
    code_folding: hide
---

```{r setup, message = FALSE, warning = FALSE}
library(tidyverse)
library(modelr)
library(purrr)
library(corrplot)
library(FactoMineR)
library(factoextra)
library(rpart)
library(rpart.plot)
library(randomForest)
library(party)

set.seed(1)
```


```{r include = FALSE}
#load mortality dataset
mort_data =
  read_csv("mortality_data_cleaned.csv") |> 
  janitor::clean_names() |>
  select(-group, everything())

convert_to_factor <- function(df, columns) {
  df[columns] <- lapply(df[columns], factor)
  return(df)
}

mort_tidy = 
  mort_data |>
  mutate(outcome = recode(outcome, `0` = "Alive", `1` = "Death")) |>
  convert_to_factor(c("group", "gender", "outcome", "hypertensive", 
                      "atrialfibrillation", "chd_with_no_mi", "diabetes", 
                      "deficiencyanemias", "depression", "hyperlipemia", 
                      "renal_failure", "copd"))

#select patient complications
com_tidy =
  mort_tidy |>
  select(outcome, hypertensive:copd)

#select patient vital signs
sign_tidy =
  mort_tidy |>
  select(outcome, heart_rate:ef)
```

\ \par
## 1. Correlation between Vital Signs

```{r fig.width = 10, fig.height = 10}
#show correlation
corrplot(cor(sign_tidy |> select(-outcome)), type = "upper", diag = FALSE)
```

  * The correlation plot illustrates the relationships between vital signs pairwise. 
  
  * Darker colors indicate stronger correlations, where blue signifies positive correlations and red signifies negative ones. 
  
  * Strong positive correlations were observed between variables such as `rbc` and `hematocrit`, `mcv` and `mch`, `inr` and `pt`, `pco2` and `bicarbonate`. Conversely, a strong negative correlation was found between `neutrophils` and `lymphocyte`. 
  
  * Strong positive correlations often suggest the potential for dimension reduction by merging these variables during selection. 

\ \par
## 2. Decision Trees and Random Forests: Dimension Reduction and Core Variables Analysis

\ \par
### Analysis of Patient Compllications Variables

```{r}
#decision tree for complications
tree_com = ctree(outcome ~ ., data = com_tidy)
plot(tree_com)
```

  * The decision tree employed variables `renal_failure` and `atrialfibrillation` to predict the outcome, showing strong evidence (p<0.05) for selecting these two variables. 

  * However, the probability distribution at the nodes suggests that this decision-making approach might not be sufficiently reliable. This could be due to the similar effects of each variable, making it challenging to determine the next variable for refining the decision. Alternatively, it might indicate insufficient evidence to support the selection of other variables (p-values not meeting the criteria). 

  * Furthermore, the binary nature of these patient complications variables posed certain challenges in constructing the classification tree, making characteristics of the variables hard to analyze. 

```{r}
#random forest for complications
rf_com = randomForest(outcome ~ ., data = com_tidy)
importance(rf_com) |> knitr::kable(digits = 3)
varImpPlot(rf_com)
```

  * The random forest yielded a list of variables ranked by their importance based on `mean decrease in Gini index`, a parameter calculated by assessing the influence of variables on the nodes of the classification tree. 

  * In descending order of importance, the top 5 variables are: `hyperlipemia`, `atrialfibrillation`, `renal_failure`, `deficiencyanemias`, `diabetes`. 

\ \par
### Analysis of Patient Vital Signs Variables

```{r fig.width = 10, fig.height = 10}
#decision tree for vital signs
tree_sign_r = rpart(outcome ~ ., data = sign_tidy, method = "class", control = rpart.control(cp = 0.02))
print(tree_sign_r)
rpart.plot(tree_sign_r)

tree_sign_c = ctree(outcome ~ ., data = sign_tidy)
plot(tree_sign_c)
```

  * The analysis of continuous variables related to vital signs resulted in the successful establishment of classification criteria by the decision tree, incorporating a total of 7 variables. It formed a 4-level classification structure with reasonably distributed probabilities across nodes.

  * The primary level classification variable is `anion_gap`. 

  * The secondary level classification variables are `blood_calcium`, `bicarbonate`. 

  * The tertiary level classification variables are `lactic_acid`, `leucocyte`, `respiratory_rate`. 

  * The forth level classification variable is `pt`. 

```{r}
#random forest for vital signs
rf_sign = randomForest(outcome ~ ., data = sign_tidy)
importance(rf_sign) |> knitr::kable(digits = 3)
varImpPlot(rf_sign)
```

  * Ranked by the `mean decrease gini`, the top 5 variables are: `anion_gap`, `bicarbonate`, `lactic_acid`, `lymphocyte`, `leucocyte`. 

  * The results obtained from the random forest exhibit a overlap with the decision tree outcomes. In fact, random forest is an extension of decision trees that address over-fitting issues inherent in decision tree model. Therefore, we tend to prioritize the key variables identified by the random forest. 

```{r}
#pca using R build-in function prcomp()
pca_res_sign = prcomp(sign_tidy |> select(-outcome), scale. = TRUE)
summary(pca_res_sign)

#pca using packages `FactoMineR`, `factoextra`
res_pca_sign = PCA(sign_tidy |> select(-outcome), scale.unit = TRUE, graph = TRUE)
#PCA() function would do the data standardization automatically.
#keep 5 dimensions as result.

get_eigenvalue(res_pca_sign)
#`variance.percent` explains the percentage of change. 70% would be adequate.

#scree plot
fviz_eig(res_pca_sign, addlabels = TRUE, ylim = c(0, 20))
#visualize the contributions of each dimension.

#result
var_sign <- get_pca_var(res_pca_sign) 
var_sign$coord
#shown by correlation

fviz_pca_var(res_pca_sign, col.var = "contrib", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
#based on above results (`var_com$coord`)
#basically,
#(a) positive correlated variables are grouped together,
#(b) negative correlated variables are located on opposite sides of the origin,
#(c) the distance between the variable and the origin measures the quality of the variable. Variables that are far from the origin are well represented.

corrplot(var_sign$cos2, is.corr=FALSE)
#shows the quality of variables.

var_sign$contrib
#shown by contribution

fviz_contrib(res_pca_sign, choice = "var", axes = 1, top = 10)
fviz_contrib(res_pca_sign, choice = "var", axes = 1:5, top = 10)

#colored individuals
fviz_pca_ind(res_pca_sign,
             geom.ind = "point",
             col.ind = as.character(sign_tidy$outcome),
             palette = c("#00AFBB", "#FC4E07"),
             addEllipses = TRUE,
             legend.title = "Groups")
```


